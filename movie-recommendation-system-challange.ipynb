{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\n\n# Set shell to show all lines of output\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from time import time\nimport math\nimport heapq  # for retrieval topK\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nimport scipy.sparse as sp\nnp.random.seed(42)\n\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\nimport matplotlib.pyplot as plt\n\n# PyTorch imports\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(0)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/movies-dataset'\nINPUT_PATH = '../input/movies-dataset/user_ratedmovies-timestamps.dat'\nOUTPUT_PATH_TRAIN = 'movielens_train_rating'\nOUTPUT_PATH_TEST = 'movielens_test_rating'\nUSER_EMBED_PATH = '../input/recommendation-assignment/user_embedding.npy'\nMOVIE_EMBED_PATH = '../input/recommendation-assignment/movie_embedding.npy'\nUSER_LABEL_MAPPING = '../input/recommendation-assignment/user_label_mapping.p'\nMOVIE_LABEL_MAPPING = '../input/recommendation-assignment/movie_label_mapping.p'\nLABEL_MOVIE_MAPPING = '../input/recommendation-assignment/label_movie_mapping.p'\nUSER_MOVIE_MAPPING = '../input/recommendation-assignment/user_movie_mapping.p'\nID_TITLE_MAPPING = '../input/recommendation-assignment/id_title_mapping.p'\nkernel_type = \"movie_recommend_model\"\n\n# CUDA for PyTorch\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nepochs = 30\nbatch_size = 256\n# first layer is the concatenation of user and item embeddings\nlayers = [64, 32, 16]\nweight_decay = 0.00001\n# Number of negative instances to pair with a positive instance while training\nnum_neg_train = 4\n# Number of negative instances to pair with a positive instance while testing\nnum_neg_test = 100\nlr = 0.001\ndropout = 0\nlearner = 'adam'\nout = 1","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"genre = pd.read_csv('../input/movies-dataset/movie_genres.dat', sep=\"\\t\", engine='python')\ngenre.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"   movieID      genre\n0        1  Adventure\n1        1  Animation\n2        1   Children\n3        1     Comedy\n4        1    Fantasy","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieID</th>\n      <th>genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Adventure</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Animation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Children</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Fantasy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_dict = {}\nfor grp in genre.groupby('movieID'):\n    grp = grp[1]\n    mid = grp['movieID'].iloc[0]\n    genres = list(grp['genre'].unique())\n    genre_dict[mid] = genres","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_df = pd.read_csv('../input/movies-dataset/movies.dat', sep=\"\\t\", engine='python')\nid_title_mapping = pd.Series(movies_df.title.values, index=movies_df.id).to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf = pd.read_csv(INPUT_PATH, sep=\"\\t\", engine='python')\ndf.head()","execution_count":21,"outputs":[{"output_type":"stream","text":"CPU times: user 6.75 s, sys: 186 ms, total: 6.93 s\nWall time: 6.99 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"   userID  movieID  rating      timestamp\n0      75        3     1.0  1162160236000\n1      75       32     4.5  1162160624000\n2      75      110     4.0  1162161008000\n3      75      160     2.0  1162160212000\n4      75      163     4.0  1162160970000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>movieID</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>75</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1162160236000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75</td>\n      <td>32</td>\n      <td>4.5</td>\n      <td>1162160624000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>75</td>\n      <td>110</td>\n      <td>4.0</td>\n      <td>1162161008000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75</td>\n      <td>160</td>\n      <td>2.0</td>\n      <td>1162160212000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75</td>\n      <td>163</td>\n      <td>4.0</td>\n      <td>1162160970000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"uid_lbl_mapping = {uid:idx+1 for idx, uid in enumerate(df.userID.unique())}\nmid_lbl_mapping = {mid:idx+1 for idx, mid in enumerate(df.movieID.unique())}\nlbl_mid_mapping = {idx+1:mid for idx, mid in enumerate(df.movieID.unique())}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_movie_mapping = {}\n\nfor grp in df.groupby('userID'):\n    user_df = grp[1]\n    user_df = user_df.sort_values('timestamp')\n    uid = user_df['userID'].iloc[0]\n    mids = list(user_df['movieID'].values)\n    user_movie_mapping[uid] = mids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save user and movie mappings\npickle.dump(uid_lbl_mapping, open(USER_LABEL_MAPPING, \"wb\"))\npickle.dump(mid_lbl_mapping, open(MOVIE_LABEL_MAPPING, \"wb\"))\npickle.dump(lbl_mid_mapping, open(LABEL_MOVIE_MAPPING, \"wb\"))\npickle.dump(user_movie_mapping, open(USER_MOVIE_MAPPING, \"wb\"))\npickle.dump(id_title_mapping, open(ID_TITLE_MAPPING, \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_test_df(df):\n    df.sort_values(by=['timestamp'], inplace=True)\n    \n    # mark last user rating as False, all others to True\n    last_user_rating_mask = df.duplicated(subset='userID', keep='last')\n    \n    # All transactions in train are sorted by timestamp,  \n    # We want items marked with a False in test dataset(last user rating)\n    train_df = df[last_user_rating_mask]\n    test_df = df[~last_user_rating_mask]\n    \n    train_df.sort_values(by=['userID', 'timestamp'], inplace=True)\n    test_df.sort_values(by=['userID', 'timestamp'], inplace=True)\n    \n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_train_test_df():\n    # make the dataset\n    df = pd.read_csv(INPUT_PATH, sep=\"\\t\", engine='python')\n    df['userID'] = df['userID'].map(uid_lbl_mapping)\n    df['movieID'] = df['movieID'].map(mid_lbl_mapping)\n    df['rating'] = 1\n    \n    # make the dataset\n    train_df, test_df = get_train_test_df(df)\n    \n    # save train and test datasets\n    train_df.to_csv(OUTPUT_PATH_TRAIN, header=False, index=False, sep='\\t')\n    test_df.to_csv(OUTPUT_PATH_TEST, header=False, index=False, sep='\\t')\n    \n    print(\"Dataset shape = {}\".format(df.shape))\n    print(\"Train size = {}, Test size = {}\".format(train_df.shape[0], test_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsave_train_test_df()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MovieLensDataset(Dataset):\n    def __init__(self, num_negatives_train=5, num_negatives_test=100):\n        self.train_matrix = self.load_rating_train_file_as_Matrix(OUTPUT_PATH_TRAIN)\n        self.num_users, self.num_items = self.train_matrix.shape\n         # make training set with negative sampling\n        self.user_input, self.item_input, self.ratings = self.get_train_instances(self.train_matrix, num_negatives_train)\n        # make testing set with negative sampling\n        self.testRatings = self.load_rating_test_file_as_list(OUTPUT_PATH_TEST)\n        self.testNegatives = self.create_negative_file(num_samples=num_negatives_test)\n        \n        assert len(self.testRatings) == len(self.testNegatives)\n        \n    def __len__(self):\n        return len(self.user_input)\n    \n    def __getitem__(self, index):\n        # get the train data\n        user_id = self.user_input[index]\n        item_id = self.item_input[index]\n        rating = self.ratings[index]\n        \n        return {\n            'user_id': user_id,\n            'item_id': item_id,\n            'rating': rating\n        }\n        \n    def load_rating_train_file_as_Matrix(self, filename):\n        num_users, num_items = 0, 0\n        with open(filename, \"r\") as f:\n            line = f.readline()\n            while line != None and line != \"\":\n                arr = line.split(\"\\t\")\n                u, i = int(arr[0]), int(arr[1])\n                num_users = max(num_users, u)\n                num_items = max(num_items, i)\n                line = f.readline()\n\n        # Construct matrix\n        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n        with open(filename, \"r\") as f:\n            line = f.readline()\n            while line != None and line != \"\":\n                arr = line.split(\"\\t\")\n                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n                if (rating > 0):\n                    mat[user, item] = 1.0\n                line = f.readline()\n                \n        return mat\n    \n    def get_train_instances(self, train, num_negatives):\n        user_input, item_input, ratings = [], [], []\n        num_users, num_items = train.shape\n        for (u, i) in train.keys():\n            # positive instance\n            user_input.append(u)\n            item_input.append(i)\n            ratings.append(1)\n            # negative instances\n            for _ in range(num_negatives):\n                j = np.random.randint(1, num_items)\n                while (u, j) in train:\n                    j = np.random.randint(1, num_items)\n                user_input.append(u)\n                item_input.append(j)\n                ratings.append(0)\n                \n        return user_input, item_input, ratings\n    \n    def load_rating_test_file_as_list(self, filename):\n        ratingList = []\n        with open(filename, \"r\") as f:\n            line = f.readline()\n            while line != None and line != \"\":\n                arr = line.split(\"\\t\")\n                user, item = int(arr[0]), int(arr[1])\n                ratingList.append([user, item])\n                line = f.readline()\n                \n        return ratingList\n    \n    def create_negative_file(self, num_samples=100):\n        negativeList = []\n        for user_item_pair in self.testRatings:\n            user = user_item_pair[0]\n            item = user_item_pair[1]\n            negatives = []\n            for t in range(num_samples):\n                j = np.random.randint(1, self.num_items)\n                while (user, j) in self.train_matrix or j == item:\n                    j = np.random.randint(1, self.num_items)\n                negatives.append(j)\n            negativeList.append(negatives)\n        return negativeList","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model, dataset, topK):\n    testRatings = dataset.testRatings\n    testNegatives = dataset.testNegatives\n\n    hits, ndcgs = [], []\n    for idx in range(len(testRatings)):\n        (hr, ndcg) = eval_one_rating(idx, topK, model, dataset, testRatings, testNegatives)\n        hits.append(hr)\n        ndcgs.append(ndcg)\n    return (hits, ndcgs)\n\ndef eval_one_rating(idx, topK, model, dataset, testRatings, testNegatives):\n    rating = testRatings[idx]\n    items = testNegatives[idx]\n    u = rating[0]\n\n    gtItem = rating[1]\n    items.append(gtItem)\n    # Get prediction scores\n    map_item_score = {}\n    users = np.full(len(items), u, dtype='int32')\n\n    feed_dict = {\n        'user_id': users,\n        'item_id': np.array(items),\n    }\n    \n    predictions = model.predict(feed_dict)\n    \n    for i in range(len(items)):\n        item = items[i]\n        map_item_score[item] = predictions[i]\n\n    # Evaluate top rank list\n    ranklist = heapq.nlargest(topK, map_item_score, key=map_item_score.get)\n    hr = getHitRatio(ranklist, gtItem)\n    ndcg = getNDCG(ranklist, gtItem)\n    return (hr, ndcg)\n\n\ndef getHitRatio(ranklist, gtItem):\n    for item in ranklist:\n        if item == gtItem:\n            return 1\n    return 0\n\n\ndef getNDCG(ranklist, gtItem):\n    for i in range(len(ranklist)):\n        item = ranklist[i]\n        if item == gtItem:\n            return math.log(2) / math.log(i+2)\n    return 0\n\ndef test_model(model, dataset, topK):\n    # put the model in eval mode before testing\n    model.eval()\n    t1 = time()\n    (hits, ndcgs) = evaluate_model(model, dataset, topK)\n    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n    print('Eval: HR = %.4f, NDCG = %.4f [%.1f s]' % (hr, ndcg, time()-t1))\n    \n    return hr, ndcg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MovieRecommendationModel(nn.Module):\n    def __init__(self, num_users, num_items, layers=[16, 8], dropout=0.0):\n        super().__init__()\n        assert (layers[0] % 2 == 0), \"layers[0] must be an even number\"\n        self.dropout = dropout\n        \n        # user and item embedding layers\n        embedding_dim = int(layers[0]/2)\n        self.user_embedding = torch.nn.Embedding(num_users, embedding_dim)\n        self.item_embedding = torch.nn.Embedding(num_items, embedding_dim)\n        \n        self.fc_layers = torch.nn.ModuleList()\n        \n        # hidden dense layers\n        for _, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n            \n        # final layer\n        self.output_layer = torch.nn.Linear(layers[-1], 1)\n        \n    def forward(self, data):\n        users = data['user_id']\n        items = data['item_id']\n        user_embedding = self.user_embedding(users)\n        item_embedding = self.item_embedding(items)\n        \n        # concatenate user and item embeddings to form input\n        x = torch.cat([user_embedding, item_embedding], 1)\n        \n        for idx, _ in enumerate(range(len(self.fc_layers))):\n            x = self.fc_layers[idx](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout)\n            \n        logit = self.output_layer(x)\n        rating = torch.sigmoid(logit)\n        return rating\n    \n    def predict(self, feed_dict):\n        # return the score, inputs and outputs are numpy arrays\n        for key in feed_dict:\n            if type(feed_dict[key]) != type(None):\n                feed_dict[key] = torch.from_numpy(\n                    feed_dict[key]).to(dtype=torch.long, device=device)\n        output_scores = self.forward(feed_dict)\n        \n        return output_scores.cpu().detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, epoch_no, device, verbose=True):\n    print(\"=========================================\")\n    print(\"Epoch = {}\".format(epoch_no))\n    t1 = time()\n    epoch_loss = []\n    \n    model.train()\n    \n    for feed_dict in data_loader:\n        for key in feed_dict:\n            if type(feed_dict[key]) != type(None):\n                feed_dict[key] = feed_dict[key].to(dtype = torch.long, device = device)\n        \n        prediction = model(feed_dict)\n        rating = feed_dict['rating']\n        \n        rating = rating.float().view(prediction.size())  \n        loss = loss_fn(prediction, rating)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n        \n    epoch_loss = np.mean(epoch_loss)\n    \n    if verbose:\n        print(\"Epoch completed {:.1f} s\".format(time() - t1))\n        print(\"Train Loss: {}\".format(epoch_loss))\n        \n    return epoch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    early_stopping = False\n    early_stopping_epochs = 5\n    early_stopping_counter = 0\n    \n    best_hr, best_ndcg, best_iter = 0.0, 0.0, 0\n    topK = 10\n    model_file = f'{kernel_type}.pth'\n    t1 = time()\n    \n    movie_dataset = MovieLensDataset(num_negatives_train=num_neg_train, num_negatives_test=num_neg_test)\n    train, testRatings, testNegatives = movie_dataset.train_matrix, movie_dataset.testRatings , movie_dataset.testNegatives\n    num_users, num_items = train.shape\n    \n    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n          % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n    \n    train_loader = DataLoader(movie_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    \n    model = MovieRecommendationModel(num_users, num_items, layers=layers, dropout=dropout)\n    model.to(device)\n    \n    loss_fn = torch.nn.BCELoss()\n    \n    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n    \n    # Record performance\n    hr_list = []\n    ndcg_list = []\n    BCE_loss_list = []\n    \n    # Check Init performance\n    hr, ndcg = test_model(model, movie_dataset, topK)\n    hr_list.append(hr)\n    ndcg_list.append(ndcg)\n    BCE_loss_list.append(1)\n    \n    for epoch in range(epochs):\n        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, epoch, device)\n        hr, ndcg = test_model(model, movie_dataset, topK)\n        \n        hr_list.append(hr)\n        ndcg_list.append(ndcg)\n        BCE_loss_list.append(train_loss)\n        \n        if hr > best_hr:\n            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n            torch.save(model.state_dict(), model_file)\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter = early_stopping_counter + 1\n            \n        if (early_stopping) and (early_stopping_counter == early_stopping_epochs):\n            print(\"Early stopping on epoch:\", epoch)\n            break\n            \n            \n    best_iter = np.argmax(np.array(hr_list))\n    best_hr = hr_list[best_iter]\n    best_ndcg = ndcg_list[best_iter]\n    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %\n          (best_iter, best_hr, best_ndcg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = MovieRecommendationModel(num_users=2114, num_items=10110, layers=layers, dropout=dropout)\nmodel1.load_state_dict(torch.load(\"./movie_recommend_model.pth\"))\n\nuser_embeddings = model1.user_embedding.weight.detach().cpu().numpy()\nmovie_embeddings = model1.item_embedding.weight.detach().cpu().numpy()\n\nnp.save(USER_EMBED_PATH, user_embeddings)\nnp.save(MOVIE_EMBED_PATH, movie_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embeddings Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_dim(weights, components = 3, method = 'tsne'):\n    \"\"\"Reduce dimensions of embeddings\"\"\"\n    if method == 'tsne':\n        return TSNE(components, metric = 'cosine').fit_transform(weights)\n    elif method == 'umap':\n        # Might want to try different parameters for UMAP\n        return UMAP(n_components=components, metric = 'cosine', \n                    init = 'random', n_neighbors = 5).fit_transform(weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmovie_r = reduce_dim(movie_embeddings, components = 2, method = 'tsne')\nmovie_r.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"InteractiveShell.ast_node_interactivity = 'last'\n\nplt.figure(figsize = (10, 8))\nplt.plot(movie_r[:, 0], movie_r[:, 1], 'r.')\nplt.xlabel('TSNE 1'); \nplt.ylabel('TSNE 2'); \nplt.title('Movie Embeddings Visualized with TSNE');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recommendation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the KNN model\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend_by_movieid(movie_id):\n    movie_embeddings = np.load(MOVIE_EMBED_PATH)\n    mid_lbl_mapping = pickle.load(open(MOVIE_LABEL_MAPPING, \"rb\"))\n    lbl_mid_mapping = pickle.load(open(LABEL_MOVIE_MAPPING, \"rb\"))\n    id_title_mapping = pickle.load(open(ID_TITLE_MAPPING, \"rb\"))\n    \n    movie_label = mid_lbl_mapping.get(movie_id)\n    movie_embedding = movie_embeddings[movie_label]\n    \n    clf = KNeighborsClassifier(n_neighbors=11)\n    clf.fit(movie_embeddings, np.arange(len(movie_embeddings)))\n    \n    distances, indices = clf.kneighbors(movie_embedding.reshape(1, -1), n_neighbors=10)\n    distances, indices = zip(*sorted(zip(distances[0], indices[0])))\n    distances, indices = list(distances), list(indices)\n    \n    sorted_movie_ids = [lbl_mid_mapping[m_idx] for m_idx in indices if m_idx != 0]\n    recommend_movies = [id_title_mapping[mid] for mid in sorted_movie_ids]\n    \n    print(\"Given movie:\", id_title_mapping[movie_id])\n    print(\"Recommended movies:\", recommend_movies)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_by_movieid(60)","execution_count":25,"outputs":[{"output_type":"stream","text":"Given movie: The Indian in the Cupboard\nRecommended movies: ['The Indian in the Cupboard', 'The Borrowers', '102 Dalmatians', 'Race to Witch Mountain', 'The Little Rascals', 'The Fox and the Hound', 'The AbsentMinded Professor', 'Operation Dumbo Drop', 'Holes', 'The Adventures of Rocky & Bullwinkle']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend_by_last_viewed(user_id):\n    # load user and movie embeddings\n    movie_embeddings = np.load(MOVIE_EMBED_PATH)\n    \n    # load user, movie and user_movie mappings\n    uid_lbl_mapping = pickle.load(open(USER_LABEL_MAPPING, \"rb\"))\n    mid_lbl_mapping = pickle.load(open(MOVIE_LABEL_MAPPING, \"rb\"))\n    lbl_mid_mapping = pickle.load(open(LABEL_MOVIE_MAPPING, \"rb\"))\n    user_movie_mapping = pickle.load(open(USER_MOVIE_MAPPING, \"rb\"))\n    id_title_mapping = pickle.load(open(ID_TITLE_MAPPING, \"rb\"))\n    \n    # last 5 watched movies by user\n    user_last_watched_movies = user_movie_mapping[user_id][-5:]\n    user_watched_movies = user_movie_mapping[user_id]\n    \n    movies = list(mid_lbl_mapping.keys())\n    user_unwatched_movies = list(set(movies) - set(user_watched_movies))\n    user_unwatched_movies_idxs = [mid_lbl_mapping[mid] for mid in user_unwatched_movies]\n    \n    clf = KNeighborsClassifier(n_neighbors=11)\n    unwatched_movie_embeddings = movie_embeddings[user_unwatched_movies_idxs]\n    clf.fit(unwatched_movie_embeddings, user_unwatched_movies_idxs)\n    \n    m_dist, m_idx = [], []\n    for movie_id in user_last_watched_movies:\n        top_2 = 0\n        movie_label = mid_lbl_mapping.get(movie_id)\n        movie_embedding = movie_embeddings[movie_label]\n        distances, indices = clf.kneighbors(movie_embedding.reshape(1, -1), n_neighbors=10)\n        distances, indices = zip(*sorted(zip(distances[0], indices[0])))\n        distances, indices = list(distances), list(indices)\n        \n        for i, indx in enumerate(indices):\n            if indx not in m_idx and indx != 0 and top_2 < 2:\n                top_2 += 1\n                m_idx.append(indx)\n                m_dist.append(distances[i])\n        \n    m_dist, sorted_movie_indexes = zip(*sorted(zip(m_dist, m_idx)))\n    m_dist, sorted_movie_indexes = list(m_dist), list(sorted_movie_indexes)\n    sorted_movie_ids = [lbl_mid_mapping[m_idx] for m_idx in sorted_movie_indexes]\n    \n    # recommend top 10 movies\n    recommend_movies = [id_title_mapping[mid] for mid in sorted_movie_ids[:10]]\n    print(\"Recommended Movies:\", recommend_movies)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_by_last_viewed(75)","execution_count":28,"outputs":[{"output_type":"stream","text":"Recommended Movies: ['37ï¿½2 le matin', 'Tears of the Sun', 'Austin Powers: The Spy Who Shagged Me', 'The Sure Thing', 'Giulietta degli spiriti', 'Riding the Bullet', \"Porky's Revenge\", 'Duplex', 'Down from the Mountain', 'Apocalypse Now']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend_by_userid(user_id):\n    # load user and movie embeddings\n    user_embeddings = np.load(USER_EMBED_PATH)\n    movie_embeddings = np.load(MOVIE_EMBED_PATH)\n    \n    # load user, movie and user_movie mappings\n    uid_lbl_mapping = pickle.load(open(USER_LABEL_MAPPING, \"rb\"))\n    mid_lbl_mapping = pickle.load(open(MOVIE_LABEL_MAPPING, \"rb\"))\n    lbl_mid_mapping = pickle.load(open(LABEL_MOVIE_MAPPING, \"rb\"))\n    user_movie_mapping = pickle.load(open(USER_MOVIE_MAPPING, \"rb\"))\n    id_title_mapping = pickle.load(open(ID_TITLE_MAPPING, \"rb\"))\n    \n    user_label = uid_lbl_mapping.get(user_id)\n    user_embedding = user_embeddings[user_label]\n    \n    user_watched_movies = user_movie_mapping[user_id]\n    movies = list(mid_lbl_mapping.keys())\n    user_unwatched_movies = list(set(movies) - set(user_watched_movies))\n    user_unwatched_movies_labels = [mid_lbl_mapping[mid] for mid in user_unwatched_movies]\n    \n    clf = KNeighborsClassifier(n_neighbors=11)\n    unwatched_movie_embeddings = movie_embeddings[user_unwatched_movies_labels]\n    clf.fit(unwatched_movie_embeddings, user_unwatched_movies_labels)\n    \n    distances, indices = clf.kneighbors(user_embedding.reshape(1, -1), n_neighbors=10)\n    distances, indices = zip(*sorted(zip(distances[0], indices[0])))\n    distances, indices = list(distances), list(indices)\n    \n    sorted_movie_ids = [lbl_mid_mapping[m_idx] for m_idx in indices if m_idx != 0]\n    \n    recommend_movies = [id_title_mapping[mid] for mid in sorted_movie_ids]\n    print(\"Recommended movies:\", recommend_movies)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_by_userid(75)","execution_count":7,"outputs":[{"output_type":"stream","text":"Recommended movies: ['Manito', 'High Art', 'Copying Beethoven', 'American Psycho II: All American Girl', \"The River's Edge\", 'La mentale', 'Calamari Union', 'Festen', 'Love the Hard Way', 'Cool Hand Luke']\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}